\chapter{Tokenizer}
\label{ch:tokenizer}

In order to make the eventual parsing process a lot easier, the compiler first splits up the input into tokens. Because this is a handwritten tokenizer, the choice has been made to do this in two steps, one running directly after the other per token. The first generates the tokens all non-keyword tokens, and tokenizes keywords as identifiers. The second stage then filters out those identifiers that are actually keywords, and changes the produced token to reflect that.

This approach makes identifier tokenizing a lot easier, since at first there is no need to consider the possibility that an identifier is a keyword. This has the nice sideeffect that, as far as the first stage is concerned, the type of the token can be deduced with only one or two characters lookahead. This avoids the need of implementing a full on state machine.

The second stage is simple since the entire text string of the identifier is then available, and a string comparison will suffice.

\section{Tokens}
Datastructure for tokens is simple, just type with associated data:
<<token struct>>=
struct token
{
	token_type type;
	source_position position;
	unsigned int intval;
	char charval;
	std::string id;
};
@

Token type is identified with a giant enum. The enum here just declares the enum itself, various TOKEN\_ definitions are at their generation sites in the tokenizer. The only token type define here is TOKEN\_EOF, because there is no place that is more natural.
Various token types:
<<token types>>=
enum token_type
{
	<<token type bracket>>
	<<token type math operators>>
	<<token type comparison and assign>>
	<<token type logic operators>>
	<<token type other simple tokens>>
	<<token type keywords>>
	<<token types with associated values>>
	
	TOKEN_EOF,					// End of file token
};
@

\section{Tokenizing}

Going from plain text to a token stream is a two step process. First we generate tokens, with everything that is id-like becomes an id token. In this step the first character is almost always enough to determine the type of the entire token, so we just do a huge switch on it.
<<generate basic token>>=
char next_char;
char value;
switch(cur_char)
{
<<tokenize brackets>>
<<tokenize math operators>>
<<tokenize comparison and assign>>
<<tokenize logic operators>>
<<tokenize other 1 character symbols>>
<<tokenizing types with associated values>>
}
@

\subsection{Brackets}
Bracket tokens are simple, one character with no surprises:
<<token type bracket>>=
TOKEN_SQUAREBRACKET_LEFT,	// [
TOKEN_SQUAREBRACKET_RIGHT,	// ]
TOKEN_CURLYBRACKET_LEFT,	// {
TOKEN_CURLYBRACKET_RIGHT,	// }
TOKEN_ROUNDBRACKET_LEFT,	// (
TOKEN_ROUNDBRACKET_RIGHT,	// )
@
<<tokenize brackets>>=
case '[':
	cur_token.type = TOKEN_SQUAREBRACKET_LEFT;
	break;
case ']':
	cur_token.type = TOKEN_SQUAREBRACKET_RIGHT;
	break;
case '{':
	cur_token.type = TOKEN_CURLYBRACKET_LEFT;
	break;
case '}':
	cur_token.type = TOKEN_CURLYBRACKET_RIGHT;
	break;
case '(':
	cur_token.type = TOKEN_ROUNDBRACKET_LEFT;
	break;
case ')':
	cur_token.type = TOKEN_ROUNDBRACKET_RIGHT;
	break;
@
<<print brackets>>=
case TOKEN_SQUAREBRACKET_LEFT:
	return "[";
case TOKEN_SQUAREBRACKET_RIGHT:
	return "]";
case TOKEN_CURLYBRACKET_LEFT:
	return "{";
case TOKEN_CURLYBRACKET_RIGHT:
	return "}";
case TOKEN_ROUNDBRACKET_LEFT:
	return "(";
case TOKEN_ROUNDBRACKET_RIGHT:
	return ")";
@

\subsection{Math operators}
The same holds for the math operators:
<<token type math operators>>=
TOKEN_PLUS,					// +
TOKEN_MINUS,				// -
TOKEN_TIMES,				// *
TOKEN_DIV,					// /
TOKEN_MOD,					// %
@
<<tokenize math operators>>=
case '+':
	cur_token.type = TOKEN_PLUS;
	break;
case '-':
	cur_token.type = TOKEN_MINUS;
	break;
case '*':
	cur_token.type = TOKEN_TIMES;
	break;
case '/':
	cur_token.type = TOKEN_DIV;
	break;
case '%':
	cur_token.type = TOKEN_MOD;
	break;
@
<<print math operators>>=
case TOKEN_PLUS:
	return "+";
case TOKEN_MINUS:
	return "-";
case TOKEN_TIMES:
	return "*";
case TOKEN_DIV:
	return "/";
case TOKEN_MOD:
	return "%";
@

\subsection{Comparison operators}
The comparison operators are a little trickier. Both assignment and equality start with an equals sign, and both smaller than and greater than have also or equal variants. However maximal matching works for spl:

<<token type comparison and assign>>=
TOKEN_ASSIGN,				// =

TOKEN_EQ,					// ==
TOKEN_LT,					// <
TOKEN_GT,					// >
TOKEN_LE,					// <=
TOKEN_GE,					// >=
@

<<tokenize comparison and assign>>=
case '=':
	next_char = tok_getchar();
	if (next_char == '=')
	{
		cur_token.type = TOKEN_EQ;
	}
	else
	{
		cur_token.type = TOKEN_ASSIGN;
		tok_ungetchar(next_char);
	}
	break;
case '<':
	next_char = tok_getchar();
	if (next_char == '=')
	{
		cur_token.type = TOKEN_LE;
	}
	else
	{
		cur_token.type = TOKEN_LT;
		tok_ungetchar(next_char);
	}
	break;
case '>':
	next_char = tok_getchar();
	if (next_char == '=')
	{
		cur_token.type = TOKEN_GE;
	}
	else
	{
		cur_token.type = TOKEN_GT;
		tok_ungetchar(next_char);
	}
	break;
@
<<print comparison and assign>>=
case TOKEN_ASSIGN:
	return "=";
case TOKEN_EQ:
	return "==";
case TOKEN_LT:
	return "<";
case TOKEN_LE:
	return "<=";
case TOKEN_GT:
	return ">";
case TOKEN_GE:
	return ">=";
@

\subsection{Logic operators}
The logic operators are a bit weird. Most of them are double character tokens, but we know what the second character needs to be entirely from the first. Hence we just verify that they are there (or report error). The exclamation mark can be two things, depending on the next character.

<<token type logic operators>>=
TOKEN_AND,					// &&
TOKEN_OR,					// ||
TOKEN_NOT,					// !
TOKEN_NE,					// !=
@
<<tokenize logic operators>>=
case '&':
	next_char = tok_getchar();
	if (next_char != '&')
	{
		// Easy error to fix
		tok_error("Incomplete operator &, did you mean &&?");
	}
	cur_token.type = TOKEN_AND;
	break;
case '|':
	next_char = tok_getchar();
	if (next_char != '|')
	{
		// Easy error to fix
		tok_error("Incomplete operator |, did you mean ||?");
	}
	cur_token.type = TOKEN_OR;
	break;
case '!':
	next_char = tok_getchar();
	if (next_char == '=')
	{
		cur_token.type = TOKEN_NE;
	}
	else
	{
		tok_ungetchar(next_char);
		cur_token.type = TOKEN_NOT;
	}
	break;
@
<<print logic operators>>=
case TOKEN_AND:
	return "&&";
case TOKEN_OR:
	return "||";
case TOKEN_NOT:
	return "!";
case TOKEN_NE:
	return "!=";
@

\subsection{Other simple tokens}
This leaves only a few other simple (1 character) tokens.

<<token type other simple tokens>>=
TOKEN_COLON,				// :
TOKEN_DOT,					// .
TOKEN_COMMA,				// ,
TOKEN_SEMICOLON,			// ;
@
<<tokenize other 1 character symbols>>=
case ':':
	cur_token.type = TOKEN_COLON;
	break;
case ';':
	cur_token.type = TOKEN_SEMICOLON;
	break;
case '.':
	cur_token.type = TOKEN_DOT;
	break;
case ',':
	cur_token.type = TOKEN_COMMA;
	break;
@
<<print other simple tokens>>=
case TOKEN_COLON:
	return ":";
case TOKEN_DOT:
	return ".";
case TOKEN_COMMA:
	return ",";
case TOKEN_SEMICOLON:
	return ";";
@

\subsection{Compound tokens}
This leaves us with the more complicated tokens, those that actually have values attached to them. Of these, character constants are the easiest, as they will always start with a quote. Numeric and identifier tokens start with such large classes of characters that using the switch for them (though faster) is not the best way. For these we just fall back to the builtin character classification functions.
<<token types with associated values>>=
TOKEN_NUMERIC,				// Int value
TOKEN_CHARACTER,			// Char value
TOKEN_ID,					// Identifier
@
<<tokenizing types with associated values>>=
<<tokenize character constant>>
default:
	// Identifier:
	if (isalpha(cur_char))
	{
		<<tokenize identifier>>
	}
	else if (isdigit(cur_char))
	{
		<<tokenize numeric>>
	}
	else
	{
		// Totaly unrecognized character, hence error
		tok_error("Unrecognized character %c in input.", cur_char);
		
		// Try again to find token
		return tok_getfromstream();
	}
@
<<tokenize character constant>>=
case '\'':
	value = tok_getchar();
	next_char = tok_getchar();
	
	if (value == '\n')
	{
		tok_error("Open character constant at end of line.");
		tok_ungetchar(next_char);
		next_char = '\'';
	}
	
	if (value == '\\')
	{
		switch(next_char)
		{
		case '\'':
			value = '\'';
			break;
		case '\\':
			value = '\\';
			break;
		case 'n':
			value = '\n';
			break;
		case 't':
			value = '\t';
			break;
		default:
			tok_error("Escaped %c has no meaning.", next_char);
		}
		
		next_char = tok_getchar();
	}
	
	if (next_char != '\'' && value == '\'')
	{
		// Likely empty char constant given, interpret it as such
		tok_error("Empty character constant.");
		tok_ungetchar(next_char);
	}
	else if (next_char != '\'')
	{
		// Likely forgot closing quote, interpret it as such
		tok_error("Character constant not closed.");
		tok_ungetchar(next_char);
	}
	cur_token.type = TOKEN_CHARACTER;
	cur_token.charval = value;
	break;
@

Identifiers have to start with a letter, but may contain numbers after that first letter. Since the check that we start with a letter is already done, we just need isalnum here.
<<tokenize identifier>>=
cur_token.type = TOKEN_ID;
cur_token.id = "";
while (isalnum(cur_char) || cur_char == '_')
{
	cur_token.id += cur_char;
	cur_char = tok_getchar();
}
tok_ungetchar(cur_char);	// if it is not part of the id, 
                        	// next character might be part of the next token.
@

Numeric tokens are stored in an integer, which means we have limited range. Hence we do an initial range check here.
<<tokenize numeric>>=
cur_token.type = TOKEN_NUMERIC;
cur_token.intval = 0;
while(isdigit(cur_char))
{
	if (cur_token.intval * 10 + (cur_char-'0') < cur_token.intval)
		tok_error("Integer constant out of bounds.");
	cur_token.intval *= 10;
	cur_token.intval += (cur_char - '0');
	cur_char = tok_getchar();
}
tok_ungetchar(cur_char);	// not part of number, 
                        	//hence might be part of next token.
@
Printing is relatively easy:
<<print compound tokens>>=
case TOKEN_ID:
	return t.id;
case TOKEN_CHARACTER:
	return string(1, t.charval);
case TOKEN_NUMERIC:
	{
		ostringstream res;
		res << t.intval;
		return res.str();
	}
@

\subsection{Keywords}
As a second stage, keywords are filtered out from the id tokens:
<<token type keywords>>=
TOKEN_IF,					// if
TOKEN_ELSE,					// else
TOKEN_WHILE,				// while
TOKEN_RETURN,				// return
@
<<filter keywords>>=
if (cur_token.type == TOKEN_ID)
{
	if (cur_token.id == "if")
		cur_token.type = TOKEN_IF;
	if (cur_token.id == "else")
		cur_token.type = TOKEN_ELSE;
	if (cur_token.id == "while")
		cur_token.type = TOKEN_WHILE;
	if (cur_token.id == "return")
		cur_token.type = TOKEN_RETURN;
}
@
<<print keywords>>=
case TOKEN_IF:
	return "if";
case TOKEN_ELSE:
	return "else";
case TOKEN_WHILE:
	return "while";
case TOKEN_RETURN:
	return "return";
@

\subsection{Whitespace, comments and end of file}
Of course, this is not the whole story, we also need to handle whitespace and comments and end of file:
<<filter whitespace>>=
char cur_char = tok_getchar();
token cur_token;
while (isspace(cur_char))
	cur_char = tok_getchar();
@

<<filter comments>>=
if (cur_char == '/')
{
	char next_char = tok_getchar();
	if (next_char == '/')
	{
		<<skip line comment>>
		
		return tok_getfromstream();
	}
	else if (next_char == '*')
	{
		<<skip multiline comment>>
		return tok_getfromstream();
	}
	tok_ungetchar(next_char);
}
@

The tricky bit about a multiline comment is that we might run out of file before it is closed. This specs isn't clear on whether that should be considered an error, hence we just warn.

<<skip multiline comment>>=
source_position comment_startpos = tok_curpos();
next_char = tok_getchar();
while (1)
{
	while(next_char != '*' && next_char != 0)
		next_char = tok_getchar();
	<<check end of file in multiline>>
	next_char = tok_getchar();
	if (next_char == '/')
		break;
}
@
<<check end of file in multiline>>=
if (next_char == 0)
{
	tok_warning_pos(comment_startpos, "Unclosed multiline comment");
	cur_token.type = TOKEN_EOF;
	cur_token.position = tok_curpos();
	return cur_token;
}
@

Line comments can end on a line. Normal behaviour for a line comment is to read the ending newline and discard it. However, when there is no such newline, but an eof terminating the comment, we know the complete token. We could handle this case by just pushing back the read eof and calling getfromstream again, but we might as well generate the eof token immediately.

<<skip line comment>>=
while (next_char != '\n' && next_char != 0)
	next_char = tok_getchar();

if (next_char == 0)
{
	cur_token.type = TOKEN_EOF;
	cur_token.position = tok_curpos();
	return cur_token;
}
@

<<filter end of file>>=
if (cur_char == 0)
{
	// eof
	cur_token.type = TOKEN_EOF;
	cur_token.position = tok_curpos();
	return cur_token;
}
@

\subsection{Final tokenizing function}
Finaly, we put it all together:
<<token from stream>>=
token tok_getfromstream()
{
	<<filter whitespace>>
	<<filter comments>>
	<<filter end of file>>
	source_position cur_pos = tok_curpos();
	<<generate basic token>>
	<<filter keywords>>
	cur_token.position = cur_pos;
	
	return cur_token;
}
@

\section{Tokenizer input interface}

The tokenizer proper uses a few function calls to manage it's input stream, and the current location associated with it, we need to implement those.

First the data about the current position:
<<tokenizer file data>>=
FILE *input_file;
string input_filename;
int lineno;
int offset;
stack<char> unget_characters;
@

The tokenizer needs a nice way to get and unget characters from the main file stream
<<tokenizer read functions>>=
char tok_getchar()
{
	if (!unget_characters.empty())
	{
		char result = unget_characters.top();
		unget_characters.pop();
		offset++;
		return result;
	}
	int input_char = fgetc(input_file);
	
	if (input_char == EOF)
		return 0;
		
	if (input_char == '\n')
	{
		lineno++;
		offset=0;
	}
	else
	{
		offset++;
	}
	return (char) input_char;
}

void tok_ungetchar(char c)
{
	unget_characters.push(c);
	offset--;
}
@

We need to be able to extract the position data in a reasonable way.
<<generate position data>>=
source_position tok_curpos()
{
	source_position result;
	result.filename = input_filename;
	result.lineno = lineno;
	result.offset = offset;
	return result;
}
@

\section{Tokenizer application interface}

We also need some interface to the rest of the compiler, this implements that.

First of all, we need to be able to point the tokenizer to an input file:
<<open input file>>=
void tok_setinput(string filename)
{
	input_filename = filename;
	input_file = fopen(filename.c_str(), "r");
	if (input_file == NULL)
		tok_fatal("Could not open input file %s", filename.c_str());
	
	// reset location data
	lineno = 1;
	offset = 0;
}
void tok_setinput_fd(string filename, FILE *desc)
{
	input_filename = filename;
	lineno = 1;
	offset = 0;
	input_file = desc;
}
@

<<tokenizer input header>>=
void tok_setinput(std::string filename);
void tok_setinput_fd(std::string filename,FILE *desc);
@

Second, the parser will need to have some interface for getting its tokens.
<<tokenizer unget data>>=
stack<token> unget_tokens;
@

<<parser tokenizer interface>>=
token tok_get()
{
	if (!unget_tokens.empty())
	{
		token res = unget_tokens.top();
		unget_tokens.pop();
		return res;
	}
	
	return tok_getfromstream();
}

void tok_unget(token t)
{
	unget_tokens.push(t);
}
@

<<tokenizer interface header>>=
token tok_get();
void tok_unget(token t);
@

\section{Tokenizer error interface}
In the tokenizer, almost all errors are generated at the point they really are, hence positions are added to errors/warnings in the actual error functions themselves.
<<tokenizer errors>>=
void tok_error(const char *message, ...)
{
	va_list args;
	va_start(args, message);
	
	eh_error(tok_curpos(), message, args);
	
	va_end(args);
}

void tok_warning_pos(source_position pos, const char *message, ...)
{
	va_list args;
	va_start(args, message);
	
	eh_warning(pos, message, args);
	
	va_end(args);
}

void tok_fatal(const char *message, ...)
{
	va_list args;
	va_start(args, message);
	
	eh_error(tok_curpos(), message, args);
	
	va_end(args);
	
	exit(1);
}
@

\section{token printing}
When generating error messages it is very usefull to have a string represtation of token to indicate to the user what went wrong.
<<token name header>>=
std::string token_name(token t);
@
<<token name>>=
string token_name(token t)
{
	switch(t.type)
	{
	<<print brackets>>
	<<print math operators>>
	<<print comparison and assign>>
	<<print logic operators>>
	<<print other simple tokens>>
	<<print keywords>>
	<<print compound tokens>>
	case TOKEN_EOF:
		return "";
	}
	return "Unknown token";
}
@

\section{header and source}
Combining the pieces:

<<token.h>>=
#ifndef TOKEN_H
#define TOKEN_H
#include "position.h"

<<token types>>
<<token struct>>

<<tokenizer input header>>
<<tokenizer interface header>>
<<token name header>>
#endif
@

<<token.cpp>>=
#include "token.h"
#include "error.h"
#include <sstream>
#include <cstdarg>
#include <cstdio>
#include <string>
#include <stack>

using namespace std;

<<tokenizer file data>>
<<tokenizer read functions>>
<<generate position data>>

<<tokenizer errors>>

<<token from stream>>

<<tokenizer unget data>>
<<parser tokenizer interface>>
<<open input file>>
<<token name>>
@
